import torch
import torch.nn as nn
import torch.optim as optim

from model import SimpleTritonModel
from data.dummy_data import get_dummy_data

# =============================================
# è¶…å‚æ•°è®¾ç½®
# =============================================
dim = 128
num_classes = 10
num_epochs = 3
lr = 1e-3
batch_size = 256

# =============================================
# æ•°æ®ç”Ÿæˆ
# =============================================
x, y = get_dummy_data(num_samples=2048, dim=dim, num_classes=num_classes)

# =============================================
# åˆ‡åˆ†ä¸º batchï¼ˆæ‰‹åŠ¨æ¨¡æ‹Ÿï¼‰
# =============================================
n_batches = x.shape[0] // batch_size
x_batches = x.split(batch_size)
y_batches = y.split(batch_size)

# =============================================
# åˆå§‹åŒ–æ¨¡å‹ä¸ä¼˜åŒ–å™¨
# =============================================
model = SimpleTritonModel(dim=dim, num_classes=num_classes).cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# =============================================
# è®­ç»ƒå¾ªç¯
# =============================================
for epoch in range(num_epochs):
    total_loss = 0.0
    print(f"\nğŸŸ¡ Epoch {epoch + 1}/{num_epochs} å¼€å§‹è®­ç»ƒ =======================")

    for batch_idx, (xb, yb) in enumerate(zip(x_batches, y_batches)):
        preds = model(xb)
        loss = criterion(preds, yb)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # æ‰“å°æ¯ä¸ª batch çš„è¾“å‡ºç¤ºä¾‹
        if batch_idx == 0:
            softmax_probs = torch.softmax(preds, dim=1)
            pred_classes = preds.argmax(dim=1)
            print(f"\nğŸ”¹ è¾“å…¥æ ·æœ¬å‰5ç»´ï¼ˆxb[0][:5]ï¼‰: {xb[0][:5].tolist()}")
            print(f"ğŸ”¹ Logitsï¼ˆpreds[0]ï¼‰:         {preds[0].tolist()}")
            print(f"ğŸ”¹ Softmax æ¦‚ç‡ï¼ˆå‰3ç±»ï¼‰:     {softmax_probs[0][:3].tolist()}")
            print(f"ğŸ”¹ é¢„æµ‹ç±»åˆ«: {pred_classes[0].item()} | çœŸå®æ ‡ç­¾: {yb[0].item()}")

        print(f"   ğŸ” Batch {batch_idx + 1}/{n_batches} - Loss: {loss.item():.4f}")

    avg_loss = total_loss / n_batches
    print(f"âœ… Epoch {epoch + 1} ç»“æŸ - å¹³å‡ Loss: {avg_loss:.4f}")
